export const metadata = {
  title: 'Responses',
  description: 'Understanding Responses from Targon',
}

# Responses

Responses are largely consistent with the OpenAI Chat API. This means that `choices` is always an array, even if the model only returns one completion. Each choice will contain a `delta` property if a stream was requested and a `message` property otherwise. This makes it easier to use the same code for all models.

At a high level, **Targon normalizes the schema across models** and providers so you only need to learn one.

## Response Body

Note that `finish_reason` will vary depending on the model provider. The `model` property tells you which model was used inside the underlying API.

Here's the response schema as a TypeScript type:

```typescript
type Response = {
  id: string;
  choices: (NonStreamingChoice | StreamingChoice | NonChatChoice)[];
  created: number; // Unix timestamp
  model: string;
  object: 'chat.completion' | 'chat.completion.chunk';
  system_fingerprint?: string; // Only present if the provider supports it
  usage?: ResponseUsage;
};

// ... (other type definitions)
```

## Example Response

```json
{
  "id": "gen-xxxxxxxxxxxxxx",
  "choices": [
    {
      "finish_reason": "stop", // Different models provide different reasons here
      "message": {
        // will be "delta" if streaming
        "role": "assistant",
        "content": "Hello there!"
      }
    }
  ],
  "usage": {
    "prompt_tokens": 0,
    "completion_tokens": 4,
    "total_tokens": 4
  },
  "model": "openai/gpt-3.5-turbo" // Could also be "anthropic/claude-2.1", etc, depending on the "model" that ends up being used
}
```

## Querying Cost and Stats

The token counts that are returned in the completions API response are NOT counted with the model's native tokenizer. Instead it uses a normalized, model-agnostic count.

For precise token accounting using the model's native tokenizer, use the `/api/v1/generation` endpoint.

You can use the returned `id` to query for the generation stats (including token counts and cost) after the request is complete.

## SSE Streaming Comments

For SSE streams, we occasionally need to send an SSE comment to indicate that Targon is processing your request. This helps prevent connections from timing out. The comment will look like this:

```
: TARGON PROCESSING
```

Comment payload can be safely ignored per the SSE specs. However, you can leverage it to improve UX as needed, e.g. by showing a dynamic loading indicator.

For more detailed information about responses, including recommended SSE client implementations, please refer to our [full responses documentation](https://targon.sybil.com/docs/responses).
